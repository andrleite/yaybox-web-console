PRoblemas para subir cliente prime


1- Calculo working hours antigo.
2- Erro no user_data para instalar salt.
3- Limite de EIP excedido. (sem permissao para abrir o chamado).
4- Problema no broker para falar com AD para liberar role de abertura de chamado na aws.
6- Problema ao criar CoreInstacia para 3 topologia quando ambiente é criado com (dev, qa, prov) Bug aberto.
7- Problema ao executar instalação do produto, não estava pegando credenciais do SITVLA.
8- Step 9 do pipeline 6 do JPS travado no salt. Precisou falhar manualmente e dar retry
9- Jps não conseguiu setar o job para Done. 2.1.8 :024 > p.error
 => {"message"=>"Transação inválida de estado para o pipeline", "code"=>"PIPELINER-10", 
 "class_name"=>"Pipeliner::Manager::InvalidPipelineStepStateTransition", "context"=>{"pipeline_step_id"=>397524, 
 "transition_message"=>"Event 'fail' cannot transition from 'failed'"}}.
10- Step 11 do pipeline 7 do jps travado no salt. Precisou falhar manualmente e dar retry
11 - 2.1.8 :035 > p.error
 => {"message"=>"Pipeline externo falhou", "code"=>"Jericore-51", "class_name"=>"ExternalPipelineError", 
 "context"=>{"message"=>"Event 'finish' cannot transition from 'done'", 
 "class_name"=>"AASM::InvalidTransition", "pipeline_id"=>133641, "pipeline_step_id"=>397535}}
12- Topologia de DEV e QA foi criado no DNS com tipo CNAME para entrada tipo A (ip).
13- SITVLA desatualizado não foi possível baixar alguns pacotes.
14- LINHA_LS=$(grep -n "LSHost" $FLUIG_PATH | cut -f 1 -d ':')s
sed -i "$LINHA_LS/value=\".*\"/value=\"54\.94\.138\.208\"/g" $FLUIG_PATH alterou o standalone.xml
15- Versao 1.5.6-EP9 não homologada!
16- Subindo na nova Versao
2.1.8 :008 > pp.error
 => {"message"=>"Pipeline externo falhou", "code"=>"Jericore-51", "class_name"=>"ExternalPipelineError", "context"=>{"message"=>"Server broke connection", "class_name"=>"RestClient::ServerBrokeConnection", "pipeline_id"=>134384, "pipeline_step_id"=>399737}}
 



* Foram 69 steps no pipeline, desses steps geram outros pipelines no jis e jps com n steps,
 hoje em dia é muito dificl rastrear exatamente onde ocorreu o problema, pois o salt mesmo executado com sucesso,
 se um passo da receita falhar, ele continua e quando vc tem um erro por falta de algum passo que o salt deveria executar 
 para rastrear onde foi isso eh muito dificil . 

 Um exemplo de um job do salt que para o jerico ocorreu com sucesso:

 [root@auto-syndic02 ~]# salt-run jobs.lookup_jid 20161012113934983362 --out=json
{
    "244-core_fluig-76a9b1b69f": {
        "cmd_|-command_execute1_|-TotvsId=$(grep TotvsId /outsourcing/totvs/license/bin/appserver/appserver.ini | cut -d '=' -f 2) && sed -i \"s/TotvsId=.*/TotvsId=$TotvsId/g\" /outsourcing/totvs/license/bin/appserver/licenseserverinstall.ini_|-run": {
            "comment": "Command \"TotvsId=$(grep TotvsId /outsourcing/totvs/license/bin/appserver/appserver.ini | cut -d '=' -f 2) && sed -i \"s/TotvsId=.*/TotvsId=$TotvsId/g\" /outsourcing/totvs/license/bin/appserver/licenseserverinstall.ini\" run",
            "_stamp": "2016-10-12T14:39:36.555524",
            "return": "Error: cmd.run",
            "name": "TotvsId=$(grep TotvsId /outsourcing/totvs/license/bin/appserver/appserver.ini | cut -d '=' -f 2) && sed -i \"s/TotvsId=.*/TotvsId=$TotvsId/g\" /outsourcing/totvs/license/bin/appserver/licenseserverinstall.ini",
            "success": false,
            "start_time": "11:39:37.661017",
            "jid": "20161012113934983362",
            "duration": 8.6,
            "result": false,
            "fun": "state.sls",
            "__run_num__": 0,
            "changes": {
                "pid": 16541,
                "retcode": 2,
                "stderr": "sed: can't read /outsourcing/totvs/license/bin/appserver/licenseserverinstall.ini: No such file or directory",
                "stdout": ""
            },
            "id": "244-core_fluig-76a9b1b69f",
            "retcode": 2
        },
        "cmd_|-command_execute3_|-LsAlias=$(grep LsAlias /outsourcing/totvs/license/bin/appserver/appserver.ini | cut -d '=' -f 2) && sed -i \"s/LsAlias=.*/LsAlias=$LsAlias/g\" /outsourcing/totvs/license/bin/appserver/licenseserverinstall.ini_|-run": {
            "comment": "Command \"LsAlias=$(grep LsAlias /outsourcing/totvs/license/bin/appserver/appserver.ini | cut -d '=' -f 2) && sed -i \"s/LsAlias=.*/LsAlias=$LsAlias/g\" /outsourcing/totvs/license/bin/appserver/licenseserverinstall.ini\" run",
            "name": "LsAlias=$(grep LsAlias /outsourcing/totvs/license/bin/appserver/appserver.ini | cut -d '=' -f 2) && sed -i \"s/LsAlias=.*/LsAlias=$LsAlias/g\" /outsourcing/totvs/license/bin/appserver/licenseserverinstall.ini",
            "start_time": "11:39:37.676635",
            "result": false,
            "duration": 6.899,
            "__run_num__": 2,
            "changes": {
                "pid": 16551,
                "retcode": 2,
                "stderr": "sed: can't read /outsourcing/totvs/license/bin/appserver/licenseserverinstall.ini: No such file or directory",
                "stdout": ""
            }
        },
        "cmd_|-command_execute2_|-LsEnv=$(grep LsEnv /outsourcing/totvs/license/bin/appserver/appserver.ini | cut -d '=' -f 2) && sed -i \"s/LsEnv=.*/LsEnv=$LsEnv/g\" /outsourcing/totvs/license/bin/appserver/licenseserverinstall.ini_|-run": {
            "comment": "Command \"LsEnv=$(grep LsEnv /outsourcing/totvs/license/bin/appserver/appserver.ini | cut -d '=' -f 2) && sed -i \"s/LsEnv=.*/LsEnv=$LsEnv/g\" /outsourcing/totvs/license/bin/appserver/licenseserverinstall.ini\" run",
            "name": "LsEnv=$(grep LsEnv /outsourcing/totvs/license/bin/appserver/appserver.ini | cut -d '=' -f 2) && sed -i \"s/LsEnv=.*/LsEnv=$LsEnv/g\" /outsourcing/totvs/license/bin/appserver/licenseserverinstall.ini",
            "start_time": "11:39:37.669754",
            "result": false,
            "duration": 6.752,
            "__run_num__": 1,
            "changes": {
                "pid": 16546,
                "retcode": 2,
                "stderr": "sed: can't read /outsourcing/totvs/license/bin/appserver/licenseserverinstall.ini: No such file or directory",
                "stdout": ""
            }
        }
    }
}

Se olharmos o erro, não achou o diretório. Esse era um job do step 18 de 22 de um pipeline do JPS gerado por 1 step do pipeline do jerico.
Agora para rastrear onde deveria ter criado esse diretório e falhar todos os steps até la é totalmente inviável e geraria um trabalho
manual tremendo e a chance de falhar algo errado ou faltar algo é enorme.

Sugiro que todo job do salt deva ser verificado se realmente ele fez o que deveria fazer. Caso de algum erro, o step do respectivo job
seja falhado. Assim não perdemos a rastreabilidade de onde foi o problema.

Hoje em um caso desses, a solução é basicamente deletar tudo e começar do zero, o que demanda um trabalho manual muito grande também.
